{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf17d65-396f-4410-a7ea-31b7e7cddc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\t-bao\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4130f0da-8d9f-4199-8270-0f50a5fbbfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_RGUQe397BeGEmCY9BEHX081lAleMnmw\n",
      "To: C:\\Users\\T-bao\\bank-full.csv\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.61M/4.61M [00:00<00:00, 5.37MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bank-full.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# Google Drive file ID\n",
    "file_id = \"1_RGUQe397BeGEmCY9BEHX081lAleMnmw\"\n",
    "# Construct the download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "# Output file path\n",
    "output = \"bank-full.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888cf84d-a376-48fe-b622-507c08f0fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with semicolon as the delimiter\n",
    "df = pd.read_csv(\"bank-full.csv\", delimiter=';')\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6af708-2b75-4a06-8ddb-453563f3ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'age', 'job', 'marital', 'education', 'balance', 'housing', 'contact', \n",
    "    'day', 'month', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'y'\n",
    "]\n",
    "\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89e6853-d70d-44c9-8170-c45cae54ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)  # 0.25 * 0.8 = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091940cb-d787-4f89-8390-5a709cf54c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no' 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in 'y' before mapping\n",
    "print(df_train['y'].unique())\n",
    "unique_values = df_train['y'].unique()\n",
    "if set(unique_values) == {'yes', 'no'}:\n",
    "    # Perform mapping\n",
    "    df_train['y'] = df_train['y'].map({'yes': 1, 'no': 0})\n",
    "else:\n",
    "    print('y is already mapped to numeric values.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e570de-dc2f-409e-9f0e-f65058554801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28481d2a-3265-498a-a317-8920b89f9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for balance: 0.589\n",
      "AUC for day: 0.526\n",
      "AUC for duration: 0.815\n",
      "AUC for previous: 0.599\n"
     ]
    }
   ],
   "source": [
    "numerical_vars = ['balance', 'day', 'duration', 'previous']\n",
    "for var in numerical_vars:\n",
    "    auc = roc_auc_score(df_train['y'], df_train[var])\n",
    "    # If AUC < 0.5, invert the variable\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(df_train['y'], -df_train[var])\n",
    "    print(f'AUC for {var}: {auc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07fce6ae-6716-4f34-bdbc-37279345fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the original dataset: 45211\n",
      "Total NaN values in 'y': 0\n"
     ]
    }
   ],
   "source": [
    "# Check if the original 'y' column has any NaN values\n",
    "print(f\"Total rows in the original dataset: {len(df)}\")\n",
    "print(f\"Total NaN values in 'y': {df['y'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1719570a-9fc6-48f8-8530-9826c92ecfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 27126 rows where 'y' is NaN in the training set\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample sequence X is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Step 6: One-hot encoding with DictVectorizer\u001b[39;00m\n\u001b[0;32m     30\u001b[0m dv \u001b[38;5;241m=\u001b[39m DictVectorizer(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mdv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dicts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m X_val \u001b[38;5;241m=\u001b[39m dv\u001b[38;5;241m.\u001b[39mtransform(val_dicts)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:316\u001b[0m, in \u001b[0;36mDictVectorizer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    Like fit(X) followed by transform(X), but does not require\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        Feature vectors; always 2-d.\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:263\u001b[0m, in \u001b[0;36mDictVectorizer._transform\u001b[1;34m(self, X, fitting)\u001b[0m\n\u001b[0;32m    260\u001b[0m     indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(indices))\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indptr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample sequence X is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(indices, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintc)\n\u001b[0;32m    266\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(indptr) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocab))\n",
      "\u001b[1;31mValueError\u001b[0m: Sample sequence X is empty."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Ensure the 'y' column is correctly mapped to 1/0 and check for missing values\n",
    "df_train['y'] = df_train['y'].map({'yes': 1, 'no': 0})\n",
    "df_val['y'] = df_val['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Step 2: Check for any NaN values in 'y' and drop rows with missing target values\n",
    "if df_train['y'].isna().sum() > 0:\n",
    "    print(f\"Dropping {df_train['y'].isna().sum()} rows where 'y' is NaN in the training set\")\n",
    "    df_train = df_train.dropna(subset=['y'])\n",
    "\n",
    "if df_val['y'].isna().sum() > 0:\n",
    "    print(f\"Dropping {df_val['y'].isna().sum()} rows where 'y' is NaN in the validation set\")\n",
    "    df_val = df_val.dropna(subset=['y'])\n",
    "\n",
    "# Step 3: Extract the target variable 'y' after cleaning\n",
    "y_train = df_train['y'].values\n",
    "y_val = df_val['y'].values\n",
    "\n",
    "# Step 4: Remove the target variable from the feature set\n",
    "df_train = df_train.drop(columns=['y'])\n",
    "df_val = df_val.drop(columns=['y'])\n",
    "\n",
    "# Step 5: Convert to dictionaries for DictVectorizer\n",
    "train_dicts = df_train.to_dict(orient='records')\n",
    "val_dicts = df_val.to_dict(orient='records')\n",
    "\n",
    "# Step 6: One-hot encoding with DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "X_val = dv.transform(val_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281e013-fefe-46fb-8422-e3ebc1da6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Check for NaN values in y and handle missing values in X\n",
    "# Ensure 'y_train' does not contain NaN values by dropping rows with missing targets\n",
    "nan_count = np.isnan(y_train).sum()\n",
    "\n",
    "if nan_count > 0:\n",
    "    print(f\"Dropping {nan_count} rows where the target 'y' is NaN\")\n",
    "    # Drop rows where 'y' is NaN\n",
    "    X_train_cleaned = X_train[~np.isnan(y_train)]\n",
    "    y_train_cleaned = y_train[~np.isnan(y_train)]\n",
    "else:\n",
    "    X_train_cleaned = X_train\n",
    "    y_train_cleaned = y_train\n",
    "\n",
    "# Step 2: If there are missing values in X_train, I might use imputation\n",
    "imputer = SimpleImputer(strategy='mean')  # or strategy='median',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9879c0-9c86-464b-93b5-aaf00e89b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total number of rows in the training set before dropping NaN values\n",
    "total_rows = len(df_train)\n",
    "print(f\"Total rows in the training dataset: {total_rows}\")\n",
    "print(f\"Rows with NaN values in 'y': {nan_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
